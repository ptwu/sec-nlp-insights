{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Ic3TcmTnDO"
      },
      "source": [
        "# Predicting Stock Price Movement Through Natural Language Processing for SEC Filings\n",
        "\n",
        "By Kevin Zhou (klz23), Peter Wu (plw53), Ashley Young (acy39)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF8WkqkDT2j3"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "First, let us import all necessary dependencies, and do some static analysis on them so that we can deploy this code in the webapp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRsxB4dncwsk",
        "outputId": "b21ae7c6-9d95-44fb-dd92-eeebd3abccb8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import re\n",
        "import nltk\n",
        "import textblob \n",
        "from textblob import TextBlob\n",
        "import pickle as pkl\n",
        "from IPython.display import display\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import lightgbm \n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import xgboost\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unqb4nMjKcff"
      },
      "outputs": [],
      "source": [
        "# Get actual requirements.txt for use in the webapp\n",
        "from pip._internal.utils.misc import get_installed_distributions\n",
        "import sys\n",
        "#import numpy as np # imported to test whether numpy shows up, which it does!\n",
        "\n",
        "def get_imported_packages():\n",
        "    p = get_installed_distributions()\n",
        "    p = {package.key:package.version for package in p}\n",
        "\n",
        "    imported_modules = set(sys.modules.keys())\n",
        "    \n",
        "    imported_modules.remove('pip')\n",
        "\n",
        "    modules = [(m, p[m]) for m in imported_modules if p.get(m, False)]\n",
        "\n",
        "    return modules\n",
        "\n",
        "\n",
        "def generate_requirements(filepath:str, modules):\n",
        "    with open(filepath, 'w') as f:\n",
        "        for module, version in modules:\n",
        "            f.write(f\"{module}=={version}\\n\")\n",
        "\n",
        "\n",
        "generate_requirements('requirements.txt', get_imported_packages())\n",
        "!cat requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGIVDzmpfY_g"
      },
      "source": [
        "# Preparing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4z2B1vU58NW",
        "outputId": "eb589021-e0ca-4ae5-b511-c1acf229feff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ],
      "source": [
        "# Getting stock price changes (by percent of change), keys are stock tickers\n",
        "drive.mount('drive')\n",
        "price_change = pkl.load(open('drive/My Drive/ai-prac/stock_prices.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olAaijEuYaHW"
      },
      "outputs": [],
      "source": [
        "price_change30 = pkl.load(open('drive/My Drive/ai-prac/stock_prices30.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwp-QFHr53Vk"
      },
      "outputs": [],
      "source": [
        "# Reading in cik to symbol mapping\n",
        "symbol_cik = pd.read_excel('drive/My Drive/ai-prac/F500_CIKs.xlsx')\n",
        "\n",
        "# Dataframe indexed by CIK with corresponding symbol\n",
        "symbol_cik = symbol_cik.set_index(symbol_cik['CIK'])\n",
        "symbol_cik = symbol_cik.drop(columns = ['CIK'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub08lycOT5wo"
      },
      "source": [
        "## Web Scraping\n",
        "\n",
        "EDGAR-Corpus paper: https://arxiv.org/pdf/2109.14394.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA001IEydnQM"
      },
      "outputs": [],
      "source": [
        "# Scraping using edgar-crawler from the EDGAR-CORPUS paper\n",
        "!rm -rf edgar-crawler/\n",
        "!git clone https://github.com/nlpaueb/edgar-crawler.git\n",
        "!pip install -r edgar-crawler/requirements.txt\n",
        "%cd edgar-crawler/\n",
        "\n",
        "# Allow programmatic templates to file system\n",
        "# This allows us to add all the S&P 500 tickers to the config.json\n",
        "# to be extracted as 10-Ks from EDGAR.\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIHhQ7XunbWS",
        "outputId": "05d5bbea-173b-4a8d-ae4e-5b9c4869ecd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"edgar_crawler\": {\"start_year\": 2016, \"end_year\": 2020, \"quarters\": [1, 2, 3, 4], \"filing_types\": [\"10-K\"], \"user_agent\": \"Your name (your email)\", \"raw_filings_folder\": \"RAW_FILINGS\", \"indices_folder\": \"INDICES\", \"filings_metadata_file\": \"FILINGS_METADATA.csv\", \"skip_present_indices\": true, \"cik_tickers\": [\"MMM\", \"AOS\", \"ABT\", \"ABBV\", \"ABMD\", \"ACN\", \"ATVI\", \"ADM\", \"ADBE\", \"ADP\", \"AAP\", \"AES\", \"AFL\", \"A\", \"APD\", \"AKAM\", \"ALK\", \"ALB\", \"ARE\", \"ALGN\", \"ALLE\", \"LNT\", \"ALL\", \"GOOGL\", \"GOOG\", \"MO\", \"AMZN\", \"AMCR\", \"AMD\", \"AEE\", \"AAL\", \"AEP\", \"AXP\", \"AIG\", \"AMT\", \"AWK\", \"AMP\", \"ABC\", \"AME\", \"AMGN\", \"APH\", \"ADI\", \"ANSS\", \"AON\", \"APA\", \"AAPL\", \"AMAT\", \"APTV\", \"ACGL\", \"ANET\", \"AJG\", \"AIZ\", \"T\", \"ATO\", \"ADSK\", \"AZO\", \"AVB\", \"AVY\", \"BKR\", \"BALL\", \"BAC\", \"BBWI\", \"BAX\", \"BDX\", \"WRB\", \"BRK-B\", \"BBY\", \"BIO\", \"TECH\", \"BIIB\", \"BLK\", \"BK\", \"BA\", \"BKNG\", \"BWA\", \"BXP\", \"BSX\", \"BMY\", \"AVGO\", \"BR\", \"BRO\", \"BF-B\", \"CHRW\", \"CDNS\", \"CZR\", \"CPT\", \"CPB\", \"COF\", \"CAH\", \"KMX\", \"CCL\", \"CTLT\", \"CAT\", \"CBOE\", \"CBRE\", \"CDW\", \"CE\", \"CNC\", \"CNP\", \"CF\", \"CRL\", \"SCHW\", \"CHTR\", \"CVX\", \"CMG\", \"CB\", \"CHD\", \"CI\", \"CINF\", \"CTAS\", \"CSCO\", \"C\", \"CFG\", \"CLX\", \"CME\", \"CMS\", \"KO\", \"CTSH\", \"CL\", \"CMCSA\", \"CMA\", \"CAG\", \"COP\", \"ED\", \"STZ\", \"COO\", \"CPRT\", \"GLW\", \"CSGP\", \"COST\", \"CTRA\", \"CCI\", \"CSX\", \"CMI\", \"CVS\", \"DHI\", \"DHR\", \"DRI\", \"DVA\", \"DE\", \"DAL\", \"XRAY\", \"DVN\", \"DXCM\", \"FANG\", \"DLR\", \"DFS\", \"DISH\", \"DIS\", \"DG\", \"DLTR\", \"D\", \"DPZ\", \"DOV\", \"DTE\", \"DUK\", \"DD\", \"DXC\", \"EMN\", \"ETN\", \"EBAY\", \"ECL\", \"EIX\", \"EW\", \"EA\", \"ELV\", \"LLY\", \"EMR\", \"ENPH\", \"ETR\", \"EOG\", \"EPAM\", \"EQT\", \"EFX\", \"EQIX\", \"EQR\", \"ESS\", \"EL\", \"ETSY\", \"RE\", \"EVRG\", \"ES\", \"EXC\", \"EXPE\", \"EXPD\", \"EXR\", \"XOM\", \"FFIV\", \"FDS\", \"FAST\", \"FRT\", \"FDX\", \"FITB\", \"FRC\", \"FE\", \"FIS\", \"FISV\", \"FLT\", \"FMC\", \"F\", \"FTNT\", \"FTV\", \"FBHS\", \"BEN\", \"FCX\", \"GRMN\", \"IT\", \"GEN\", \"GNRC\", \"GD\", \"GE\", \"GIS\", \"GM\", \"GPC\", \"GILD\", \"GL\", \"GPN\", \"GS\", \"HAL\", \"HIG\", \"HAS\", \"HCA\", \"PEAK\", \"HSIC\", \"HSY\", \"HES\", \"HPE\", \"HLT\", \"HOLX\", \"HD\", \"HON\", \"HRL\", \"HST\", \"HWM\", \"HPQ\", \"HUM\", \"HBAN\", \"HII\", \"IBM\", \"IEX\", \"IDXX\", \"ITW\", \"ILMN\", \"INCY\", \"INTC\", \"ICE\", \"IP\", \"IPG\", \"IFF\", \"INTU\", \"ISRG\", \"IVZ\", \"IQV\", \"IRM\", \"JBHT\", \"JKHY\", \"J\", \"JNJ\", \"JCI\", \"JPM\", \"JNPR\", \"K\", \"KDP\", \"KEY\", \"KEYS\", \"KMB\", \"KIM\", \"KMI\", \"KLAC\", \"KHC\", \"KR\", \"LHX\", \"LH\", \"LRCX\", \"LW\", \"LVS\", \"LDOS\", \"LEN\", \"LNC\", \"LIN\", \"LYV\", \"LKQ\", \"LMT\", \"L\", \"LOW\", \"LUMN\", \"LYB\", \"MTB\", \"MRO\", \"MPC\", \"MKTX\", \"MAR\", \"MMC\", \"MLM\", \"MAS\", \"MA\", \"MTCH\", \"MKC\", \"MCD\", \"MCK\", \"MDT\", \"MRK\", \"META\", \"MET\", \"MTD\", \"MGM\", \"MCHP\", \"MU\", \"MSFT\", \"MAA\", \"MHK\", \"MOH\", \"TAP\", \"MDLZ\", \"MPWR\", \"MNST\", \"MCO\", \"MS\", \"MOS\", \"MSI\", \"MSCI\", \"NDAQ\", \"NTAP\", \"NFLX\", \"NWL\", \"NEM\", \"NWSA\", \"NWS\", \"NEE\", \"NKE\", \"NI\", \"NDSN\", \"NSC\", \"NTRS\", \"NOC\", \"NCLH\", \"NRG\", \"NUE\", \"NVDA\", \"NVR\", \"NXPI\", \"ORLY\", \"OXY\", \"ODFL\", \"OMC\", \"ON\", \"OKE\", \"ORCL\", \"PCAR\", \"PKG\", \"PARA\", \"PH\", \"PAYX\", \"PAYC\", \"PYPL\", \"PNR\", \"PEP\", \"PKI\", \"PFE\", \"PCG\", \"PM\", \"PSX\", \"PNW\", \"PXD\", \"PNC\", \"POOL\", \"PPG\", \"PPL\", \"PFG\", \"PG\", \"PGR\", \"PLD\", \"PRU\", \"PEG\", \"PTC\", \"PSA\", \"PHM\", \"QRVO\", \"PWR\", \"QCOM\", \"DGX\", \"RL\", \"RJF\", \"RTX\", \"O\", \"REG\", \"REGN\", \"RF\", \"RSG\", \"RMD\", \"RHI\", \"ROK\", \"ROL\", \"ROP\", \"ROST\", \"RCL\", \"SPGI\", \"CRM\", \"SBAC\", \"SLB\", \"STX\", \"SEE\", \"SRE\", \"NOW\", \"SHW\", \"SBNY\", \"SPG\", \"SWKS\", \"SJM\", \"SNA\", \"SEDG\", \"SO\", \"LUV\", \"SWK\", \"SBUX\", \"STT\", \"STE\", \"SYK\", \"SIVB\", \"SYF\", \"SNPS\", \"SYY\", \"TMUS\", \"TROW\", \"TTWO\", \"TPR\", \"TRGP\", \"TGT\", \"TEL\", \"TDY\", \"TFX\", \"TER\", \"TSLA\", \"TXN\", \"TXT\", \"TMO\", \"TJX\", \"TSCO\", \"TT\", \"TDG\", \"TRV\", \"TRMB\", \"TFC\", \"TYL\", \"TSN\", \"USB\", \"UDR\", \"ULTA\", \"UNP\", \"UAL\", \"UPS\", \"URI\", \"UNH\", \"UHS\", \"VLO\", \"VTR\", \"VRSN\", \"VRSK\", \"VZ\", \"VRTX\", \"VFC\", \"VTRS\", \"V\", \"VNO\", \"VMC\", \"WAB\", \"WBA\", \"WMT\", \"WBD\", \"WM\", \"WAT\", \"WEC\", \"WFC\", \"WELL\", \"WST\", \"WDC\", \"WRK\", \"WY\", \"WHR\", \"WMB\", \"WTW\", \"GWW\", \"WYNN\", \"XEL\", \"XYL\", \"YUM\", \"ZBRA\", \"ZBH\", \"ZION\", \"ZTS\"]}, \"extract_items\": {\"raw_filings_folder\": \"RAW_FILINGS\", \"extracted_filings_folder\": \"EXTRACTED_FILINGS\", \"filings_metadata_file\": \"FILINGS_METADATA.csv\", \"items_to_extract\": [\"1\", \"1A\", \"1B\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"7A\", \"8\", \"9\", \"9A\", \"9B\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"], \"remove_tables\": true, \"skip_extracted_filings\": true}}\n",
            "{\"edgar_crawler\": {\"start_year\": 2016, \"end_year\": 2020, \"quarters\": [1, 2, 3, 4], \"filing_types\": [\"10-K\"], \"user_agent\": \"Your name (your email)\", \"raw_filings_folder\": \"RAW_FILINGS\", \"indices_folder\": \"INDICES\", \"filings_metadata_file\": \"FILINGS_METADATA.csv\", \"skip_present_indices\": true, \"cik_tickers\": [\"MMM\", \"AOS\", \"ABT\", \"ABBV\", \"ABMD\", \"ACN\", \"ATVI\", \"ADM\", \"ADBE\", \"ADP\", \"AAP\", \"AES\", \"AFL\", \"A\", \"APD\", \"AKAM\", \"ALK\", \"ALB\", \"ARE\", \"ALGN\", \"ALLE\", \"LNT\", \"ALL\", \"GOOGL\", \"GOOG\", \"MO\", \"AMZN\", \"AMCR\", \"AMD\", \"AEE\", \"AAL\", \"AEP\", \"AXP\", \"AIG\", \"AMT\", \"AWK\", \"AMP\", \"ABC\", \"AME\", \"AMGN\", \"APH\", \"ADI\", \"ANSS\", \"AON\", \"APA\", \"AAPL\", \"AMAT\", \"APTV\", \"ACGL\", \"ANET\", \"AJG\", \"AIZ\", \"T\", \"ATO\", \"ADSK\", \"AZO\", \"AVB\", \"AVY\", \"BKR\", \"BALL\", \"BAC\", \"BBWI\", \"BAX\", \"BDX\", \"WRB\", \"BRK-B\", \"BBY\", \"BIO\", \"TECH\", \"BIIB\", \"BLK\", \"BK\", \"BA\", \"BKNG\", \"BWA\", \"BXP\", \"BSX\", \"BMY\", \"AVGO\", \"BR\", \"BRO\", \"BF-B\", \"CHRW\", \"CDNS\", \"CZR\", \"CPT\", \"CPB\", \"COF\", \"CAH\", \"KMX\", \"CCL\", \"CTLT\", \"CAT\", \"CBOE\", \"CBRE\", \"CDW\", \"CE\", \"CNC\", \"CNP\", \"CF\", \"CRL\", \"SCHW\", \"CHTR\", \"CVX\", \"CMG\", \"CB\", \"CHD\", \"CI\", \"CINF\", \"CTAS\", \"CSCO\", \"C\", \"CFG\", \"CLX\", \"CME\", \"CMS\", \"KO\", \"CTSH\", \"CL\", \"CMCSA\", \"CMA\", \"CAG\", \"COP\", \"ED\", \"STZ\", \"COO\", \"CPRT\", \"GLW\", \"CSGP\", \"COST\", \"CTRA\", \"CCI\", \"CSX\", \"CMI\", \"CVS\", \"DHI\", \"DHR\", \"DRI\", \"DVA\", \"DE\", \"DAL\", \"XRAY\", \"DVN\", \"DXCM\", \"FANG\", \"DLR\", \"DFS\", \"DISH\", \"DIS\", \"DG\", \"DLTR\", \"D\", \"DPZ\", \"DOV\", \"DTE\", \"DUK\", \"DD\", \"DXC\", \"EMN\", \"ETN\", \"EBAY\", \"ECL\", \"EIX\", \"EW\", \"EA\", \"ELV\", \"LLY\", \"EMR\", \"ENPH\", \"ETR\", \"EOG\", \"EPAM\", \"EQT\", \"EFX\", \"EQIX\", \"EQR\", \"ESS\", \"EL\", \"ETSY\", \"RE\", \"EVRG\", \"ES\", \"EXC\", \"EXPE\", \"EXPD\", \"EXR\", \"XOM\", \"FFIV\", \"FDS\", \"FAST\", \"FRT\", \"FDX\", \"FITB\", \"FRC\", \"FE\", \"FIS\", \"FISV\", \"FLT\", \"FMC\", \"F\", \"FTNT\", \"FTV\", \"FBHS\", \"BEN\", \"FCX\", \"GRMN\", \"IT\", \"GEN\", \"GNRC\", \"GD\", \"GE\", \"GIS\", \"GM\", \"GPC\", \"GILD\", \"GL\", \"GPN\", \"GS\", \"HAL\", \"HIG\", \"HAS\", \"HCA\", \"PEAK\", \"HSIC\", \"HSY\", \"HES\", \"HPE\", \"HLT\", \"HOLX\", \"HD\", \"HON\", \"HRL\", \"HST\", \"HWM\", \"HPQ\", \"HUM\", \"HBAN\", \"HII\", \"IBM\", \"IEX\", \"IDXX\", \"ITW\", \"ILMN\", \"INCY\", \"INTC\", \"ICE\", \"IP\", \"IPG\", \"IFF\", \"INTU\", \"ISRG\", \"IVZ\", \"IQV\", \"IRM\", \"JBHT\", \"JKHY\", \"J\", \"JNJ\", \"JCI\", \"JPM\", \"JNPR\", \"K\", \"KDP\", \"KEY\", \"KEYS\", \"KMB\", \"KIM\", \"KMI\", \"KLAC\", \"KHC\", \"KR\", \"LHX\", \"LH\", \"LRCX\", \"LW\", \"LVS\", \"LDOS\", \"LEN\", \"LNC\", \"LIN\", \"LYV\", \"LKQ\", \"LMT\", \"L\", \"LOW\", \"LUMN\", \"LYB\", \"MTB\", \"MRO\", \"MPC\", \"MKTX\", \"MAR\", \"MMC\", \"MLM\", \"MAS\", \"MA\", \"MTCH\", \"MKC\", \"MCD\", \"MCK\", \"MDT\", \"MRK\", \"META\", \"MET\", \"MTD\", \"MGM\", \"MCHP\", \"MU\", \"MSFT\", \"MAA\", \"MHK\", \"MOH\", \"TAP\", \"MDLZ\", \"MPWR\", \"MNST\", \"MCO\", \"MS\", \"MOS\", \"MSI\", \"MSCI\", \"NDAQ\", \"NTAP\", \"NFLX\", \"NWL\", \"NEM\", \"NWSA\", \"NWS\", \"NEE\", \"NKE\", \"NI\", \"NDSN\", \"NSC\", \"NTRS\", \"NOC\", \"NCLH\", \"NRG\", \"NUE\", \"NVDA\", \"NVR\", \"NXPI\", \"ORLY\", \"OXY\", \"ODFL\", \"OMC\", \"ON\", \"OKE\", \"ORCL\", \"PCAR\", \"PKG\", \"PARA\", \"PH\", \"PAYX\", \"PAYC\", \"PYPL\", \"PNR\", \"PEP\", \"PKI\", \"PFE\", \"PCG\", \"PM\", \"PSX\", \"PNW\", \"PXD\", \"PNC\", \"POOL\", \"PPG\", \"PPL\", \"PFG\", \"PG\", \"PGR\", \"PLD\", \"PRU\", \"PEG\", \"PTC\", \"PSA\", \"PHM\", \"QRVO\", \"PWR\", \"QCOM\", \"DGX\", \"RL\", \"RJF\", \"RTX\", \"O\", \"REG\", \"REGN\", \"RF\", \"RSG\", \"RMD\", \"RHI\", \"ROK\", \"ROL\", \"ROP\", \"ROST\", \"RCL\", \"SPGI\", \"CRM\", \"SBAC\", \"SLB\", \"STX\", \"SEE\", \"SRE\", \"NOW\", \"SHW\", \"SBNY\", \"SPG\", \"SWKS\", \"SJM\", \"SNA\", \"SEDG\", \"SO\", \"LUV\", \"SWK\", \"SBUX\", \"STT\", \"STE\", \"SYK\", \"SIVB\", \"SYF\", \"SNPS\", \"SYY\", \"TMUS\", \"TROW\", \"TTWO\", \"TPR\", \"TRGP\", \"TGT\", \"TEL\", \"TDY\", \"TFX\", \"TER\", \"TSLA\", \"TXN\", \"TXT\", \"TMO\", \"TJX\", \"TSCO\", \"TT\", \"TDG\", \"TRV\", \"TRMB\", \"TFC\", \"TYL\", \"TSN\", \"USB\", \"UDR\", \"ULTA\", \"UNP\", \"UAL\", \"UPS\", \"URI\", \"UNH\", \"UHS\", \"VLO\", \"VTR\", \"VRSN\", \"VRSK\", \"VZ\", \"VRTX\", \"VFC\", \"VTRS\", \"V\", \"VNO\", \"VMC\", \"WAB\", \"WBA\", \"WMT\", \"WBD\", \"WM\", \"WAT\", \"WEC\", \"WFC\", \"WELL\", \"WST\", \"WDC\", \"WRK\", \"WY\", \"WHR\", \"WMB\", \"WTW\", \"GWW\", \"WYNN\", \"XEL\", \"XYL\", \"YUM\", \"ZBRA\", \"ZBH\", \"ZION\", \"ZTS\"]}, \"extract_items\": {\"raw_filings_folder\": \"RAW_FILINGS\", \"extracted_filings_folder\": \"EXTRACTED_FILINGS\", \"filings_metadata_file\": \"FILINGS_METADATA.csv\", \"items_to_extract\": [\"1\", \"1A\", \"1B\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"7A\", \"8\", \"9\", \"9A\", \"9B\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"], \"remove_tables\": true, \"skip_extracted_filings\": true}}\n"
          ]
        }
      ],
      "source": [
        "config_json_str_init = \"\"\"\n",
        "{\n",
        "\t\"edgar_crawler\": {\n",
        "\t\t\"start_year\": 2016,\n",
        "\t\t\"end_year\": 2020,\n",
        "\t\t\"quarters\": [1, 2, 3, 4],\n",
        "\t\t\"filing_types\": [\"10-K\"],\n",
        "\t\t\"user_agent\": \"Your name (your email)\",\n",
        "\t\t\"raw_filings_folder\": \"RAW_FILINGS\",\n",
        "\t\t\"indices_folder\": \"INDICES\",\n",
        "\t\t\"filings_metadata_file\": \"FILINGS_METADATA.csv\",\n",
        "\t\t\"skip_present_indices\": true\n",
        "\t},\n",
        "\t\"extract_items\": {\n",
        "\t\t\"raw_filings_folder\": \"RAW_FILINGS\",\n",
        "\t\t\"extracted_filings_folder\": \"EXTRACTED_FILINGS\",\n",
        "\t\t\"filings_metadata_file\": \"FILINGS_METADATA.csv\",\n",
        "\t\t\"items_to_extract\": [\n",
        "\t\t\t\"1\", \"1A\", \"1B\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"7A\",\n",
        "\t\t\t\"8\", \"9\", \"9A\", \"9B\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"\n",
        "\t\t],\n",
        "\t\t\"remove_tables\": true,\n",
        "\t\t\"skip_extracted_filings\": true\n",
        "\t}\n",
        "}\n",
        "\"\"\"\n",
        "config_json = json.loads(config_json_str_init)\n",
        "config_json[\"edgar_crawler\"][\"cik_tickers\"] = [ticker for ticker in price_change]\n",
        "\n",
        "config_json_str = json.dumps(config_json)\n",
        "\n",
        "print(config_json_str)\n",
        "print(config_json_str, file=open('config.json', 'w'))\n",
        "\n",
        "!cat config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWU2aiSuyRP_",
        "outputId": "c760ffe9-186e-4ba0-82b3-91ac9cdbc714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.json  edgar_crawler.py  __init__.py  logger.py  README.md\n",
            "datasets     extract_items.py  LICENSE\t    logs       requirements.txt\n",
            "Saving log to /content/edgar-crawler/logs\n",
            "\n",
            "Downloading EDGAR Index files\n",
            "2016_QTR1.tsv downloaded\n",
            "2016_QTR2.tsv downloaded\n",
            "2016_QTR3.tsv downloaded\n",
            "2016_QTR4.tsv downloaded\n",
            "2017_QTR1.tsv downloaded\n",
            "2017_QTR2.tsv downloaded\n",
            "2017_QTR3.tsv downloaded\n",
            "2017_QTR4.tsv downloaded\n",
            "2018_QTR1.tsv downloaded\n",
            "2018_QTR2.tsv downloaded\n",
            "2018_QTR3.tsv downloaded\n",
            "2018_QTR4.tsv downloaded\n",
            "2019_QTR1.tsv downloaded\n",
            "2019_QTR2.tsv downloaded\n",
            "2019_QTR3.tsv downloaded\n",
            "2019_QTR4.tsv downloaded\n",
            "2020_QTR1.tsv downloaded\n",
            "2020_QTR2.tsv downloaded\n",
            "2020_QTR3.tsv downloaded\n",
            "2020_QTR4.tsv downloaded\n",
            "NumExpr defaulting to 2 threads.\n",
            "\n",
            "Downloading 2384 filings...\n",
            "\n",
            "100%|███████████████████████████████████████████████████████████| 2384/2384 [34:30<00:00,  1.15it/s]\n",
            "\n",
            "Filings metadata exported to /content/edgar-crawler/datasets/FILINGS_METADATA.csv\n"
          ]
        }
      ],
      "source": [
        "!ls\n",
        "!python edgar_crawler.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe-Uhy6Yj16s"
      },
      "outputs": [],
      "source": [
        "!python extract_items.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEJxvAQmn6sj",
        "outputId": "941a8ef7-6914-4f87-8621-623cc90df641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1018724_10K_2018_0001018724-19-000004.json\n",
            "1018724_10K_2019_0001018724-20-000004.json\n",
            "320193_10K_2019_0000320193-19-000119.json\n",
            "320193_10K_2020_0000320193-20-000096.json\n"
          ]
        }
      ],
      "source": [
        "!ls datasets/EXTRACTED_FILINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IReMNmzCT9Xo"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liarCdYGebej"
      },
      "outputs": [],
      "source": [
        "# Lemmatizing the text\n",
        "def lemmatize(text):\n",
        "  words = nltk.word_tokenize(text)\n",
        "  lem_words = [WordNetLemmatizer().lemmatize(s.lower()) for s in words]\n",
        "  result = ' '.join(lem_words)\n",
        "  return result\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap0MLZCTgXHO"
      },
      "outputs": [],
      "source": [
        "# Remove everything but the word stems for each word in the text\n",
        "def stem(text):\n",
        "  ps = PorterStemmer()\n",
        "  words = nltk.word_tokenize(text)\n",
        "  stemmed_words = [ps.stem(s) for s in words]\n",
        "  result = ' '.join(stemmed_words)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM4AXvc6gjvf"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuation from the text\n",
        "def removePunc(text):\n",
        "  return re.sub(r'[^\\w\\s]', '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9WDMQHtu8Jk"
      },
      "outputs": [],
      "source": [
        "def removeBreaks(text):\n",
        "  text = text.replace('\\\\n', '')\n",
        "  text = text.replace('\\\\t', '')\n",
        "  text = text.replace('\\\\r', '')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hjpt78_g66o"
      },
      "source": [
        "## Sentiment Analysis\n",
        "FinBERT Paper: https://arxiv.org/pdf/1908.10063.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9zLgS1IjeKX"
      },
      "outputs": [],
      "source": [
        "def compute_finbert_probabilities(text):\n",
        "  \"\"\"\n",
        "  Uses FinBERT via REST to compute the sentiment analysis logits\n",
        "  given a string of text\n",
        "\n",
        "  Return format:\n",
        "  {\"POSITIVE\": number, \"NEGATIVE\": number, \"NEUTRAL\": number}\n",
        "  \"\"\"\n",
        "  url = \"https://finbert3.p.rapidapi.com/sentiment/en\"\n",
        "\n",
        "  payload = {\"text\": text}\n",
        "  headers = {\n",
        "    \"content-type\": \"application/json\",\n",
        "    \"X-RapidAPI-Key\": \"3ccadbbca0msh476e5295a20c3d1p15ea35jsn05ff8341a2d1\",\n",
        "    \"X-RapidAPI-Host\": \"finbert3.p.rapidapi.com\"\n",
        "  }\n",
        "\n",
        "  response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
        "\n",
        "  res_dict = json.loads(response.text)\n",
        "  return res_dict[\"sentiment_probabilities\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HjWcyw0u3vf"
      },
      "outputs": [],
      "source": [
        "# Useful constants \n",
        "FILINGS_BASE_PATH = \"drive/My Drive/extracted-data/json/\"\n",
        "ITEMS_TO_CONCAT = [\n",
        "   \"7\"\n",
        "]\n",
        "ITEMS_TO_DROP = [\n",
        "  \"1\", \"1A\", \"1B\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7A\",\n",
        "  \"8\", \"9\", \"9A\", \"9B\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieHuXSaWhXXJ"
      },
      "outputs": [],
      "source": [
        "# Reads in text and tokenizes the text into sentences, which are then ordered by subjectivity using the TextBlob nlp library \n",
        "def most_subjective(text):\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  scores = []\n",
        "  for sentence in sentences:\n",
        "    subjectivity = TextBlob(sentence).sentiment.subjectivity\n",
        "    scores.append((sentence, subjectivity))\n",
        "  scores.sort(key = lambda x : x[1], reverse = True)\n",
        "  total_char = 0\n",
        "  text_sents = []\n",
        "  for tup in scores:\n",
        "    total_char += len(tup[0])\n",
        "    if total_char < 2000:\n",
        "      text_sents.append(tup[0])\n",
        "    elif len(text_sents) > 0:\n",
        "      break\n",
        "  result = ' '.join(text_sents)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyFCEVjLvPTB"
      },
      "outputs": [],
      "source": [
        "\n",
        "drive.mount('drive')\n",
        "\n",
        "# load 10-ks from Colab's /content/edgar-crawler/datasets\n",
        "# load 10-ks from Colab's /content/edgar-crawler/datasets\n",
        "count = 0\n",
        "# iterate through 10-K filings \n",
        "for json_file in os.listdir(FILINGS_BASE_PATH):\n",
        "  absolute_path = FILINGS_BASE_PATH + json_file\n",
        "  # Load json file into a python dictionary\n",
        "  with open(absolute_path) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "  # Create a pandas dataframe from the dictionary\n",
        "  df = pd.DataFrame.from_dict(data, orient='index')\n",
        "\n",
        "  # Transpose the dataframe so that the json data is in one row\n",
        "  df = df.transpose()\n",
        "  if (len(main_df.loc[(main_df['cik'] == df['cik'][0]) & (main_df['filing_date'] == df['filing_date'][0])]) != 0):\n",
        "    continue\n",
        "\n",
        "  # drop unneeded columns\n",
        "  df.drop(columns=['company', 'filing_type', 'period_of_report', 'sic', \n",
        "                   'state_of_inc', 'state_location', 'fiscal_year_end', \n",
        "                   'filing_html_index', 'htm_filing_link',\n",
        "                   'complete_text_filing_link', 'filename', 'filing_html_index'],\n",
        "          inplace=True)\n",
        "  df['text'] = ''\n",
        "\n",
        "  # iterate through the MD&A sections \n",
        "  for item_name in ITEMS_TO_CONCAT:\n",
        "    col_name = \"item_\" + item_name\n",
        "\n",
        "    if col_name in df:\n",
        "      df['text'][0] = df['text'][0] + df[col_name][0]\n",
        "      df['text'] = removeBreaks((df['text']))\n",
        "      df.drop(columns=[col_name], inplace=True)\n",
        "\n",
        "  # process the MD&A text block\n",
        "  df['text'][0] = most_subjective(df['text'][0])\n",
        "  df['text'] = lemmatize((df['text'][0]))\n",
        "  df['text'] = stem(df['text'][0])\n",
        "  df['text'] = removePunc(df['text'][0])\n",
        "  \n",
        "  dt = datetime.strptime(df['filing_date'][0], '%Y-%m-%d')\n",
        "  \n",
        "  next_year = dt.year + 1\n",
        "  cik = int(df['cik'][0])\n",
        "\n",
        "  ticker = symbol_cik.loc[cik]['Symbol']\n",
        "  ticker = ticker.replace(\".\", \"-\")\n",
        "  if (isinstance(ticker, pd.Series)):\n",
        "    continue\n",
        "  stock_change = price_change[ticker][next_year]\n",
        "  df['stock_change'] = stock_change\n",
        "  \n",
        "  sentiment_scores = compute_finbert_probabilities(str(df['text']))\n",
        "  positivity = sentiment_scores['POSITIVE']\n",
        "  neutral = sentiment_scores['NEUTRAL']\n",
        "  negativity = sentiment_scores['NEGATIVE']\n",
        "  df['positivity'] = positivity\n",
        "  df['neutral'] = neutral\n",
        "  df['negativity'] = negativity\n",
        "  main_df = pd.concat([df, main_df])\n",
        "  print(\"COUNT \" + str(count))\n",
        "  display(df) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mZtiWy9sUS-",
        "outputId": "dd554621-b9c6-4ab6-a5cd-42e2a0ae83e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Saving sentiment scores to drive as a CSV\n",
        "main_df = main_df.drop_duplicates(\n",
        "  subset = ['cik', 'filing_date'],\n",
        "  keep = 'last').reset_index(drop = True)\n",
        "for drop_item in ITEMS_TO_DROP:\n",
        "    main_df.drop(columns=[\"item_\" + drop_item],\n",
        "          inplace=True, errors='ignore')\n",
        "drive.mount('drive')\n",
        "main_df.to_csv('filings_sentiments.csv')\n",
        "!cp filings_sentiments.csv \"drive/My Drive/ai-prac\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xa7UuitD434"
      },
      "source": [
        "## Regression Modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scgjtMAOBkKR"
      },
      "outputs": [],
      "source": [
        "# Read in the csv to retrieve the created dataset with sentiment scores \n",
        "df = pd.read_csv('drive/My Drive/ai-prac/filings_sentiments_stock.csv')\n",
        "df['net_positivity'] = df['positivity'] - df['negativity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2t8rpFV2o1W"
      },
      "outputs": [],
      "source": [
        "# read in the stock price change over 30 days\n",
        "price_change_filing = pd.read_csv('drive/My Drive/ai-prac/stock_prices_filing.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1_uykSwNYLd"
      },
      "outputs": [],
      "source": [
        "# read in the stock price change over 7 days\n",
        "price_change_filing7 = pd.read_csv('drive/My Drive/ai-prac/stock_prices_filing_7.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQGJc3uDNYa-"
      },
      "outputs": [],
      "source": [
        "# read in the stock price change over 15 days \n",
        "price_change_filing15 = pd.read_csv('drive/My Drive/ai-prac/stock_prices_filing_15.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS-U0_SB44Hd"
      },
      "outputs": [],
      "source": [
        "# adding in stock price change over 7 days, 15 days, and 30 days to the data set\n",
        "df['stock_change_filing'] = price_change_filing['Stock Change']\n",
        "df['stock_change_filing7'] = price_change_filing7['Stock Change']\n",
        "df['stock_change_filing15'] = price_change_filing15['Stock Change']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBihamTcD32U"
      },
      "outputs": [],
      "source": [
        "# Setting up train and test splits\n",
        "'''\n",
        "df['net_positivity'] = df['net_positivity'] \n",
        "df['positivity'] = df['positivity'] \n",
        "df['negativity'] = df['negativity'] \n",
        "df['neutral'] = df['neutral'] \n",
        "'''\n",
        " \n",
        "X = df[['positivity', 'neutral', 'negativity']]\n",
        "Y = df['stock_change_filing']\n",
        "\n",
        "# train test split \n",
        "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size =0.20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Sogao7kHD3Z"
      },
      "outputs": [],
      "source": [
        "# Linear Regression\n",
        "LR = LinearRegression()\n",
        "LR.fit(x_train, y_train)\n",
        "predictions = LR.predict(x_test)\n",
        "print(r2_score(y_test, predictions))\n",
        "\n",
        "pkl.dump(LR, open('drive/My Drive/ai-prac/LR.pkl', 'wb'))\n",
        "# print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj0WlIRsL2h8"
      },
      "outputs": [],
      "source": [
        "# Light GBM\n",
        "\n",
        "lgbm = LGBMRegressor()\n",
        "lgbm.fit(x_train, y_train)\n",
        "lgbm.score(x_test, y_test)\n",
        "predictions = lgbm.predict(x_test)\n",
        "print(r2_score(y_test, predictions))\n",
        "\n",
        "pkl.dump(lgbm, open('drive/My Drive/ai-prac/lgbm.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZdzZGjMYjoJ"
      },
      "outputs": [],
      "source": [
        "# Light GBM Graphing\n",
        "\n",
        "predictions = lgbm.predict(x_test)\n",
        "plt.ylabel('Actual Stock Change Percent')\n",
        "plt.title('Actual vs Predicted Values')\n",
        "plt.xlabel('Predicted Stock Change Percent')\n",
        "plt.scatter(100 * predictions, 100 * y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ2vO6ZAhr90"
      },
      "outputs": [],
      "source": [
        "plt.ylabel('Predicted Stock Change Percent')\n",
        "plt.title('Positivity Score vs Predicted Stock Change Percent')\n",
        "plt.xlabel('Positivity Score')\n",
        "plt.scatter(x_test['positivity'], predictions * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSX8_bLCCkw9"
      },
      "outputs": [],
      "source": [
        "# LR Graphing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = LR.predict(x_test)\n",
        "plt.ylabel('Actual Stock Change Percent')\n",
        "plt.title('Actual vs Predicted Values')\n",
        "plt.xlabel('Predicted Stock Change Percent')\n",
        "plt.scatter(100 * predictions, 100 * y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxUqs2UEYz5L"
      },
      "outputs": [],
      "source": [
        "plt.ylabel('Predicted Stock Change Percent')\n",
        "plt.title('Positivity Score vs Predicted Stock Change Percent')\n",
        "plt.xlabel('Positivity Score')\n",
        "plt.scatter(x_test['positivity'], predictions * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-oVFQsNFI_c"
      },
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "RF = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
        "RF.fit(x_train, y_train) \n",
        "predictions = RF.predict(x_test)\n",
        "print(r2_score(y_test, predictions))\n",
        "\n",
        "pkl.dump(RF, open('drive/My Drive/ai-prac/RF.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vauPPSfxj8xO"
      },
      "outputs": [],
      "source": [
        "#RF Graphing\n",
        "predictions = RF.predict(x_test)\n",
        "plt.ylabel('Actual Stock Change Percent')\n",
        "plt.title('Actual vs Predicted Values')\n",
        "plt.xlabel('Predicted Stock Change Percent')\n",
        "plt.scatter(100 * predictions, 100 * y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne8_eWTKR7Js"
      },
      "outputs": [],
      "source": [
        "plt.ylabel('Predicted Stock Change Percent')\n",
        "plt.title('Positivity Score vs Predicted Stock Change Percent')\n",
        "plt.xlabel('Positivity Score')\n",
        "plt.scatter(x_test['positivity'], predictions * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-3mx3jqJuLF"
      },
      "outputs": [],
      "source": [
        "# XGBoost\n",
        "import xgboost \n",
        "xgb = xgboost.XGBRegressor(objective ='reg:squarederror',\n",
        "                  n_estimators = 100, seed = 123)\n",
        "xgb.fit(x_train, y_train)\n",
        "xgb.score(x_test, y_test)\n",
        "predictions = xgb.predict(x_test)\n",
        "print(r2_score(y_test, predictions))\n",
        "pkl.dump(xgb, open('drive/My Drive/ai-prac/xgb.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ShUVJzFVwlU"
      },
      "outputs": [],
      "source": [
        "#XGB Graphing\n",
        "predictions = xgb.predict(x_test)\n",
        "plt.ylabel('Actual Stock Change Percent')\n",
        "plt.title('Actual vs Predicted Values')\n",
        "plt.xlabel('Predicted Stock Change Percent')\n",
        "plt.scatter(100 * predictions, 100 * y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aW4v1FJWCXz"
      },
      "outputs": [],
      "source": [
        "plt.ylabel('Predicted Stock Change Percent')\n",
        "plt.title('Positivity Score vs Predicted Stock Change Percent')\n",
        "plt.xlabel('Positivity Score')\n",
        "plt.scatter(x_test['positivity'], predictions * 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# average sentiment scores across the dataset\n",
        "print((df['positivity']).mean())\n",
        "print((df['negativity']).mean())\n",
        "print((df['neutral'].mean()))"
      ],
      "metadata": {
        "id": "xE47JDFk0KNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MsASUliMLrH9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}